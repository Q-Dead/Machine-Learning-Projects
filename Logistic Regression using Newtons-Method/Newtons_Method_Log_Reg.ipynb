{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from LogRegNewtonsMethod import NMLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabeates_dt = datasets.load_diabetes()\n",
    "X, y = diabeates_dt.data , diabeates_dt.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=46, train_size=0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CLIENT\\Desktop\\projects\\Machine-Learning-Projects\\Logistic Regression using Newtons-Method\\LogRegNewtonsMethod.py:30: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (88,10) (88,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m newtons_model \u001b[38;5;241m=\u001b[39m NMLogisticRegression()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnewtons_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\CLIENT\\Desktop\\projects\\Machine-Learning-Projects\\Logistic Regression using Newtons-Method\\LogRegNewtonsMethod.py:19\u001b[0m, in \u001b[0;36mNMLogisticRegression.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(_)\n\u001b[1;32m---> 19\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mn) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     hessian  \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mn) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhessian(X, y)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(hessian) \u001b[38;5;241m*\u001b[39m gradient)\n",
      "File \u001b[1;32mc:\\Users\\CLIENT\\Desktop\\projects\\Machine-Learning-Projects\\Logistic Regression using Newtons-Method\\LogRegNewtonsMethod.py:38\u001b[0m, in \u001b[0;36mNMLogisticRegression.gradient\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     36\u001b[0m thetaTX \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthetaTranposeX(X)\n\u001b[0;32m     37\u001b[0m yhat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(thetaTX)\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(X\u001b[38;5;241m.\u001b[39mT, (\u001b[43myhat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m))\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (88,10) (88,) "
     ]
    }
   ],
   "source": [
    "newtons_model = NMLogisticRegression()\n",
    "newtons_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 88)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = np.array(X_train) @ np.array(X_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01353015,  0.0042157 , -0.00189012, ..., -0.00565628,\n",
       "         0.00736091, -0.00305354],\n",
       "       [ 0.0042157 ,  0.01841269,  0.00720602, ..., -0.01475763,\n",
       "        -0.00513847,  0.00468447],\n",
       "       [-0.00189012,  0.00720602,  0.02846449, ..., -0.01182869,\n",
       "        -0.01228882,  0.00907821],\n",
       "       ...,\n",
       "       [-0.00565628, -0.01475763, -0.01182869, ...,  0.02234542,\n",
       "         0.01359138, -0.00470768],\n",
       "       [ 0.00736091, -0.00513847, -0.01228882, ...,  0.01359138,\n",
       "         0.03643637, -0.00543788],\n",
       "       [-0.00305354,  0.00468447,  0.00907821, ..., -0.00470768,\n",
       "        -0.00543788,  0.00767736]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 88)\n",
      "(88, 88)\n",
      "(88, 88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CLIENT\\AppData\\Local\\Temp\\ipykernel_24276\\3043423329.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  ll = np.sum(y * z - np.log(1 + np.exp(z)))\n",
      "C:\\Users\\CLIENT\\AppData\\Local\\Temp\\ipykernel_24276\\3043423329.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m X \u001b[38;5;241m=\u001b[39m X_train\n\u001b[0;32m     52\u001b[0m y \u001b[38;5;241m=\u001b[39m y_train\n\u001b[1;32m---> 54\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mnewtons_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal weights:\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights)\n",
      "Cell \u001b[1;32mIn[5], line 37\u001b[0m, in \u001b[0;36mnewtons_method\u001b[1;34m(X, y, iterations)\u001b[0m\n\u001b[0;32m     34\u001b[0m H \u001b[38;5;241m=\u001b[39m hessian(X, y, weights)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Newton's update: weights = weights + H^-1 * grad\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m H_inv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Inverse of the Hessian\u001b[39;00m\n\u001b[0;32m     38\u001b[0m weights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(H_inv, grad)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Log-likelihood after the update\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\CLIENT\\Desktop\\projects\\Machine-Learning-Projects\\Logistic Regression using Newtons-Method\\.venv\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:615\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    612\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call\u001b[38;5;241m=\u001b[39m_raise_linalgerror_singular, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    614\u001b[0m               over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 615\u001b[0m     ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\CLIENT\\Desktop\\projects\\Machine-Learning-Projects\\Logistic Regression using Newtons-Method\\.venv\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:104\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Logistic regression log-likelihood (cost) function\n",
    "def log_likelihood(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    ll = np.sum(y * z - np.log(1 + np.exp(z)))\n",
    "    return ll\n",
    "\n",
    "# Gradient of the log-likelihood\n",
    "def gradient(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = sigmoid(z)\n",
    "    return np.dot(X.T, (y - predictions))\n",
    "\n",
    "# Hessian matrix (second-order derivatives)\n",
    "def hessian(X, y, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    predictions = sigmoid(z)\n",
    "    diag = np.diag(predictions * (1 - predictions))  # Diagonal matrix\n",
    "    print(diag.shape)\n",
    "    return np.dot(np.dot(X.T, diag), X)\n",
    "\n",
    "# Newton's Method for Logistic Regression\n",
    "def newtons_method(X, y, iterations=10):\n",
    "    weights = np.zeros(X.shape[1])  # Initialize weights to zeros\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Compute the gradient and Hessian\n",
    "        grad = gradient(X, y, weights)\n",
    "        H = hessian(X, y, weights)\n",
    "        \n",
    "        # Newton's update: weights = weights + H^-1 * grad\n",
    "        H_inv = np.linalg.inv(H)  # Inverse of the Hessian\n",
    "        weights += np.dot(H_inv, grad)\n",
    "        \n",
    "        # Log-likelihood after the update\n",
    "        ll = log_likelihood(X, y, weights)\n",
    "        #print(f\"Iteration {i + 1}: Log-likelihood = {ll}\")\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy dataset (add intercept term manually)\n",
    "    # X = np.array([[1, 0.50], [1, 2.50], [1, 1.25], [1, 3.00], [1, 4.50]])\n",
    "    # y = np.array([0, 1, 0, 1, 1])\n",
    "    X = X_train\n",
    "    y = y_train\n",
    "\n",
    "    weights = newtons_method(X, y, iterations=5)\n",
    "    print(\"Final weights:\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.5 ],\n",
       "       [1.  , 2.5 ],\n",
       "       [1.  , 1.25],\n",
       "       [1.  , 3.  ],\n",
       "       [1.  , 4.5 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
